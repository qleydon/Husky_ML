#!/usr/bin/env python
import rospy
import os
import numpy as np
import time
import sys
sys.path.append(os.path.dirname(os.path.abspath(os.path.dirname(__file__))))

from std_msgs.msg import Float32MultiArray
from src.HUSKY_RL.Husky_ENV import Env
from src.HUSKY_RL.Husky_Memory import ReplayBuffer
from src.HUSKY_RL.SAC_Husky import SACAgent


import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim

import matplotlib.pyplot as plt



universal_seed=1234567
torch.manual_seed(universal_seed) 
np.random.seed(universal_seed)

TRAINING_EVALUATION_RATIO = 4
RUNS = 1
EPISODES_PER_RUN = 40
STEPS_PER_EPISODE = 300
run_evaluation_results=[]
run_evaluation_steps=[]
total_global=[]
total_local=[]
total_error=[]
run_steps=[]
agent_errors=[]
run_errors=[]

#device = 'cuda' if torch.cuda.is_available() else 'cpu'
device = 'cpu'

if __name__ == '__main__':
    print("starting up")
    rospy.init_node('husky_dqn')
    pub_result = rospy.Publisher('result', Float32MultiArray, queue_size=5)
    pub_get_action = rospy.Publisher('get_action', Float32MultiArray, queue_size=5)
    result = Float32MultiArray()
    get_action = Float32MultiArray()

    #state_size = 26
    action_size = 5

    env = Env(action_size)
    agent_results = []
    steps=0
    evaluation_steps=0

    for run in range(RUNS):
        agent = SACAgent(env, True)
        print('agent set up')
        run_results = []
        for episode_number in range(EPISODES_PER_RUN):
            print('\r', f'Run: {run + 1}/{RUNS} | Episode: {episode_number + 1}/{EPISODES_PER_RUN}', end=' ')
            evaluation_episode = episode_number % TRAINING_EVALUATION_RATIO == 0
            episode_reward = 0
            #state,_ = env.reset(seed=(universal_seed))
            state = env.reset()
            print('reset')

            done = False
            i = 0
            while not done and i < STEPS_PER_EPISODE:
                time.sleep(0.1)
                i+=1
                action = agent.get_next_action(state, evaluation_episode=evaluation_episode)
                next_state, reward, done = env.step(action)
                get_action.data = [action, episode_reward, reward]
                #print(action)
                #print(action.dtype)
                pub_get_action.publish(get_action)
                if evaluation_episode:
                    evaluation_steps=evaluation_steps+1
                if not evaluation_episode:
                    steps=steps+1
                    g_loss,l_loss,error=agent.train_on_transition(state, action, next_state, reward, done)
                    if type(g_loss) != int:
                        g_loss=g_loss.detach().numpy()
                        l_loss=l_loss.detach().numpy()
                        error=error.detach().numpy()
                    total_global.append(g_loss)
                    total_local.append(l_loss)
                    total_error.append(error)
                # else:
                episode_reward += reward
                state = next_state

            if i==STEPS_PER_EPISODE:
                rospy.loginfo("Time out!!")

            if not evaluation_episode:
                run_results.append(episode_reward)
                run_steps.append(steps)
            
            if  evaluation_episode:
                run_evaluation_results.append(episode_reward)
                run_evaluation_steps.append(evaluation_steps)

            pub_result.publish(result)

        agent_results.append(run_results)
    agent.save_state("experiment")

    env.close()

    n_results = EPISODES_PER_RUN // TRAINING_EVALUATION_RATIO
    results_mean = [np.mean([agent_result[n] for agent_result in agent_results]) for n in range(n_results)]
    results_std = [np.std([agent_result[n] for agent_result in agent_results]) for n in range(n_results)]
    mean_plus_std = [m + s for m, s in zip(results_mean, results_std)]
    mean_minus_std = [m - s for m, s in zip(results_mean, results_std)]

    x_vals = list(range(len(results_mean)))
    x_vals = [x_val * (TRAINING_EVALUATION_RATIO - 1) for x_val in x_vals]

    ax = plt.gca()
    ax.set_ylim([0, 300])
    ax.set_ylabel('Episode Score')
    ax.set_xlabel('Training Episode')
    ax.plot(x_vals, results_mean, label='Average Result', color='blue')
    ax.plot(x_vals, mean_plus_std, color='blue', alpha=0.1)
    ax.plot(x_vals, mean_minus_std, color='blue', alpha=0.1)
    ax.fill_between(x_vals, y1=mean_minus_std, y2=mean_plus_std, alpha=0.1, color='blue')
    plt.legend(loc='best')
    plt.show()